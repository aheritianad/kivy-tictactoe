# Backend implementation

This part contains the following

```bash
.
├── Q-learning_model.ipynb
├── json_agents
│   ├── easy.json
│   ├── hard_player1.json
│   ├── hard_player2.json
│   ├── medium_player1.json
│   └── medium_player2.json
├── q_agent_model.py
├── readme.md
├── tictactoe.py
└── utils.py
```

where each will be described bellow.

## Notebook for agent policy generator (`Q-learning_model.ipynb`)

This notebook contains a complete and step by step process on how to generate policies and store it in a json file.

## Directory of sample JSON files containing policies (`json_agents`)

The directory contains 5 files:

```bash
json_agents
├── easy.json
├── hard_player1.json
├── hard_player2.json
├── medium_player1.json
└── medium_player2.json
```

### Random policy (`easy.json`)

It is a policy where all allowed action share the same probability for each state.

### `Softmax`, medium level policies (`medium_player{}.json`)

They are policies generated by applying `softmax` on the approximated `Q-function`. Apparently, they are close to be uniform except for few states (at least in my end).

### `Argmax`, hard level policies (`hard_player{}.json`)

They are policies generated by applying `argmax` on the approximated `Q-function`. If there are more than one maximu, then it will give a uniform distribution on `indices` of `maximum`.

## Free tabular `Q`-agent implementation (`q_agent_model.py`)

This file contains the implementation of a free tabular `Q`-agent named `Player`. Furthermore, it has an implementation of  `run_episode_agent_vs_agent()` function which can be used to train two agents by letting them to compete between themselves.

## The TIC TAC TOE environment (`tictactoe.py`)

This file contain the full implementation of tictactoe environment. It is both used for training and deployment.

## Utility functions (`utils.py`)

From its name, it is used to store the utility functions such as `softmax`, write and read json files, ...
