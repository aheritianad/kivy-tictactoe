# **Backend implementation**

This directory contains the following

```bash
back
.
├── Q-learning_model.ipynb
├── __pycache__
│   ├── player_module.cpython-310.pyc
│   ├── q_agent_model.cpython-310.pyc
│   ├── tictactoe.cpython-310.pyc
│   ├── train_module.cpython-310.pyc
│   └── utils.cpython-310.pyc
├── json_agents
│   ├── easy.json
│   ├── hard_player1.json
│   ├── hard_player2.json
│   ├── medium_player1.json
│   └── medium_player2.json
├── player_module.py
├── readme.md
├── tictactoe.py
├── train_module.py
└── utils.py
```

where each will be described bellow.

## **Notebook for agent policy generator**(`Q-learning_model.ipynb`)

This notebook contains a complete and step by step process on how to generate `automatically & manually` policies and store it in a json file.

You will find all the macine learning part here.

## **Directory of sample JSON files containing policies** (`json_agents`)

The directory contains 5 files:

```bash
json_agents
├── easy.json
├── hard_player1.json
├── hard_player2.json
├── medium_player1.json
└── medium_player2.json
```

### *Random policy* (`easy.json`)

It is a policy where all allowed action share the same probability for each state.

### `Softmax`, *medium level policies* (`medium_player{}.json`)

They are policies generated by applying `softmax` on the approximated `Q-function`. Apparently, they are close to be uniform except for few states (at least in my end).

### `Argmax`, *hard level policies* (`hard_player{}.json`)

They are policies generated by applying `argmax` on the approximated `Q-function`. If there are more than one maximu, then it will give a uniform distribution on `indices` of `maximum`.

## **Player classes** (`player_module.py`)

This file contains the implementation of a free tabular `QAgent` and `HumanPlayer` classes which all inherit the `Player` parent class.

## **The TIC TAC TOE environment** (`tictactoe.py`)

This file contain the full implementation of tictactoe environment. It is both used for training and deployment.

## **Agent trainer** (`train_module.py`)

It has an implementation of  `run_episode()` and `train()` functions which can be used to train agent with another agent or a human player.

Furthermore, it contains functions for visualization evaluations during training

## **Utility functions** (`utils.py`)

From its name, it is used to store the utility functions for `softmax`, `argmax`, probability generator, write and read json files, visualization, ...
